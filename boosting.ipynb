{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees: Ensemble Methods - Boosting\n",
    "\n",
    "Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree.\n",
    "\n",
    "When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model.\n",
    "\n",
    "An ensemble of trees are built one by one and individual trees are summed sequentially. The Next tree tries to recover the loss (difference between actual and predicted values) from the previous tree.\n",
    "\n",
    " - boosting = low variance, high bias base learners\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost = Adaptive Boosting\n",
    "AdaBoost learns from the mistakes by increasing the weight of misclassified data points.\n",
    "\n",
    "Steps:\n",
    "<li> 0: Initialize the weights of data points. (e.g. data has 1000 points, each initial point would have 1/1000 = 0.001) </li>\n",
    "<li> 1: Train a decision Tree (whole dataset) </li>\n",
    "<li> 2: Calculate the weighted error rate (e) of the decision tree. </li>\n",
    "<li> 3: Calculate this decision tree’s weight in the ensemble the weight of this tree = learning rate * log( (1 — e) / e) </li> \n",
    "<br> ** The higher the weighted error of the tree, the less decision power the tree will be given during the later voting. </br>\n",
    "<br> ** The lower the weighted error of the tree, the higher decision power the tree will be given during the later voting. </br>\n",
    "\n",
    "<li> 4: Update weights of wrongly classified points. </li> \n",
    "<br> the weight of each data point stays same if the model got this data points correct.</br> \n",
    "<br> the new weight of this data point = old weight* np.exp(weight of the tree), if the model got this data point wrong </br> \n",
    "\n",
    "<li> 5: Repeat step 1 (dataset with new weights) </li>\n",
    "<li> 6: Make final prediction </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting = Gradient Descent + Boosting.\n",
    "\n",
    "\n",
    "Steps:\n",
    "<li> 1. Train a decison Tree </li> \n",
    "<li> 2. Apply the decision Tree just trained to predict. </li> \n",
    "<li> 3. Calculate the residual of this decision tree, save residual errors as new y. </li> \n",
    "<li> 4. Repeat step 1 (until no of trees set to train is attained). </li> \n",
    "<li> 5. Make the final prediction. </li> \n",
    "\n",
    "<strong>Note:</strong>\n",
    "\n",
    "<li> Gradient Boosting is prone to Over-fitting.</li>\n",
    "<li> Requires careful tuning of different hyper-parameters.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[01:19:33] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n--- 0.061842918395996094 seconds ---\n"
    },
    {
     "data": {
      "text/plain": "6.583590106471756"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset\n",
    "\n",
    "X,y = load_boston(return_X_y=True)\n",
    "\n",
    "#train,test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "#xgboost\n",
    "xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.1,n_estimators=100,n_jobs=1)\n",
    "start_time = time.time()\n",
    "\n",
    "xgbr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_predict = xgbr.predict(X_test)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
    "\n",
    "mean_squared_error(y_test,y_predict) #error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0:\tlearn: 8.7268017\ttotal: 1.14ms\tremaining: 113ms\n1:\tlearn: 8.2099159\ttotal: 2.14ms\tremaining: 105ms\n2:\tlearn: 7.7968101\ttotal: 3.11ms\tremaining: 100ms\n3:\tlearn: 7.3306938\ttotal: 4.09ms\tremaining: 98.2ms\n4:\tlearn: 6.9727966\ttotal: 4.96ms\tremaining: 94.2ms\n5:\tlearn: 6.6423222\ttotal: 5.82ms\tremaining: 91.2ms\n6:\tlearn: 6.3111468\ttotal: 6.97ms\tremaining: 92.7ms\n7:\tlearn: 6.0132589\ttotal: 7.96ms\tremaining: 91.5ms\n8:\tlearn: 5.7434365\ttotal: 8.98ms\tremaining: 90.9ms\n9:\tlearn: 5.5069788\ttotal: 9.91ms\tremaining: 89.2ms\n10:\tlearn: 5.2635150\ttotal: 11.4ms\tremaining: 92ms\n11:\tlearn: 4.9977830\ttotal: 12.8ms\tremaining: 93.7ms\n12:\tlearn: 4.8349713\ttotal: 14.1ms\tremaining: 94.2ms\n13:\tlearn: 4.6642290\ttotal: 15ms\tremaining: 92.3ms\n14:\tlearn: 4.5238923\ttotal: 15.9ms\tremaining: 90.2ms\n15:\tlearn: 4.3504734\ttotal: 16.8ms\tremaining: 88.2ms\n16:\tlearn: 4.1976125\ttotal: 17.6ms\tremaining: 86.1ms\n17:\tlearn: 4.1008217\ttotal: 18.5ms\tremaining: 84.4ms\n18:\tlearn: 3.9720598\ttotal: 19.6ms\tremaining: 83.7ms\n19:\tlearn: 3.8583548\ttotal: 20.7ms\tremaining: 82.6ms\n20:\tlearn: 3.7847079\ttotal: 21.5ms\tremaining: 81ms\n21:\tlearn: 3.6969666\ttotal: 22.7ms\tremaining: 80.6ms\n22:\tlearn: 3.6273763\ttotal: 23.7ms\tremaining: 79.2ms\n23:\tlearn: 3.5379013\ttotal: 24.6ms\tremaining: 78ms\n24:\tlearn: 3.4486629\ttotal: 26.5ms\tremaining: 79.4ms\n25:\tlearn: 3.3914426\ttotal: 27.5ms\tremaining: 78.4ms\n26:\tlearn: 3.3313331\ttotal: 28.6ms\tremaining: 77.4ms\n27:\tlearn: 3.2699625\ttotal: 29.6ms\tremaining: 76.2ms\n28:\tlearn: 3.2135969\ttotal: 30.6ms\tremaining: 74.8ms\n29:\tlearn: 3.1532429\ttotal: 31.6ms\tremaining: 73.7ms\n30:\tlearn: 3.1147866\ttotal: 32.5ms\tremaining: 72.3ms\n31:\tlearn: 3.0734658\ttotal: 33.4ms\tremaining: 70.9ms\n32:\tlearn: 3.0320804\ttotal: 34.3ms\tremaining: 69.7ms\n33:\tlearn: 2.9942668\ttotal: 35.4ms\tremaining: 68.7ms\n34:\tlearn: 2.9366193\ttotal: 36.5ms\tremaining: 67.8ms\n35:\tlearn: 2.9079950\ttotal: 37.4ms\tremaining: 66.5ms\n36:\tlearn: 2.8818085\ttotal: 38.4ms\tremaining: 65.4ms\n37:\tlearn: 2.8406535\ttotal: 39.5ms\tremaining: 64.4ms\n38:\tlearn: 2.8201948\ttotal: 41ms\tremaining: 64.1ms\n39:\tlearn: 2.7955113\ttotal: 42.1ms\tremaining: 63.2ms\n40:\tlearn: 2.7739650\ttotal: 43ms\tremaining: 61.9ms\n41:\tlearn: 2.7522597\ttotal: 43.9ms\tremaining: 60.6ms\n42:\tlearn: 2.7347178\ttotal: 45.1ms\tremaining: 59.7ms\n43:\tlearn: 2.6874730\ttotal: 46ms\tremaining: 58.6ms\n44:\tlearn: 2.6603172\ttotal: 47ms\tremaining: 57.5ms\n45:\tlearn: 2.6270805\ttotal: 47.9ms\tremaining: 56.2ms\n46:\tlearn: 2.6082981\ttotal: 48.9ms\tremaining: 55.1ms\n47:\tlearn: 2.5893968\ttotal: 49.9ms\tremaining: 54ms\n48:\tlearn: 2.5584241\ttotal: 50.9ms\tremaining: 53ms\n49:\tlearn: 2.5348454\ttotal: 52ms\tremaining: 52ms\n50:\tlearn: 2.5237038\ttotal: 53.6ms\tremaining: 51.5ms\n51:\tlearn: 2.5147706\ttotal: 55.5ms\tremaining: 51.2ms\n52:\tlearn: 2.4877019\ttotal: 56.9ms\tremaining: 50.5ms\n53:\tlearn: 2.4601382\ttotal: 58.3ms\tremaining: 49.7ms\n54:\tlearn: 2.4462375\ttotal: 59.8ms\tremaining: 48.9ms\n55:\tlearn: 2.4358497\ttotal: 61ms\tremaining: 47.9ms\n56:\tlearn: 2.4195041\ttotal: 62ms\tremaining: 46.8ms\n57:\tlearn: 2.4027655\ttotal: 63.5ms\tremaining: 46ms\n58:\tlearn: 2.3919661\ttotal: 64.6ms\tremaining: 44.9ms\n59:\tlearn: 2.3716898\ttotal: 66.8ms\tremaining: 44.5ms\n60:\tlearn: 2.3578698\ttotal: 68ms\tremaining: 43.5ms\n61:\tlearn: 2.3443480\ttotal: 68.9ms\tremaining: 42.2ms\n62:\tlearn: 2.3338409\ttotal: 69.9ms\tremaining: 41ms\n63:\tlearn: 2.3134440\ttotal: 71.1ms\tremaining: 40ms\n64:\tlearn: 2.2983038\ttotal: 72.2ms\tremaining: 38.9ms\n65:\tlearn: 2.2817354\ttotal: 73.6ms\tremaining: 37.9ms\n66:\tlearn: 2.2626640\ttotal: 74.6ms\tremaining: 36.7ms\n67:\tlearn: 2.2581283\ttotal: 75.5ms\tremaining: 35.5ms\n68:\tlearn: 2.2419380\ttotal: 76.3ms\tremaining: 34.3ms\n69:\tlearn: 2.2385696\ttotal: 77.1ms\tremaining: 33ms\n70:\tlearn: 2.2210607\ttotal: 77.9ms\tremaining: 31.8ms\n71:\tlearn: 2.2047150\ttotal: 78.6ms\tremaining: 30.6ms\n72:\tlearn: 2.2006378\ttotal: 79.4ms\tremaining: 29.4ms\n73:\tlearn: 2.1859889\ttotal: 80.1ms\tremaining: 28.2ms\n74:\tlearn: 2.1766985\ttotal: 81.1ms\tremaining: 27ms\n75:\tlearn: 2.1665081\ttotal: 81.9ms\tremaining: 25.9ms\n76:\tlearn: 2.1596869\ttotal: 82.7ms\tremaining: 24.7ms\n77:\tlearn: 2.1386075\ttotal: 83.6ms\tremaining: 23.6ms\n78:\tlearn: 2.1339705\ttotal: 85.2ms\tremaining: 22.6ms\n79:\tlearn: 2.1239353\ttotal: 86.7ms\tremaining: 21.7ms\n80:\tlearn: 2.1120295\ttotal: 87.9ms\tremaining: 20.6ms\n81:\tlearn: 2.0996166\ttotal: 88.8ms\tremaining: 19.5ms\n82:\tlearn: 2.0802002\ttotal: 89.6ms\tremaining: 18.4ms\n83:\tlearn: 2.0686744\ttotal: 90.4ms\tremaining: 17.2ms\n84:\tlearn: 2.0664424\ttotal: 91.3ms\tremaining: 16.1ms\n85:\tlearn: 2.0638875\ttotal: 92.1ms\tremaining: 15ms\n86:\tlearn: 2.0528183\ttotal: 92.8ms\tremaining: 13.9ms\n87:\tlearn: 2.0371156\ttotal: 93.6ms\tremaining: 12.8ms\n88:\tlearn: 2.0226819\ttotal: 94.3ms\tremaining: 11.7ms\n89:\tlearn: 2.0122886\ttotal: 95.1ms\tremaining: 10.6ms\n90:\tlearn: 2.0053872\ttotal: 95.9ms\tremaining: 9.48ms\n91:\tlearn: 2.0030229\ttotal: 96.6ms\tremaining: 8.4ms\n92:\tlearn: 1.9922703\ttotal: 97.4ms\tremaining: 7.33ms\n93:\tlearn: 1.9813282\ttotal: 98.4ms\tremaining: 6.28ms\n94:\tlearn: 1.9685755\ttotal: 99.7ms\tremaining: 5.25ms\n95:\tlearn: 1.9601576\ttotal: 101ms\tremaining: 4.21ms\n96:\tlearn: 1.9555227\ttotal: 102ms\tremaining: 3.16ms\n97:\tlearn: 1.9462351\ttotal: 103ms\tremaining: 2.1ms\n98:\tlearn: 1.9339477\ttotal: 104ms\tremaining: 1.05ms\n99:\tlearn: 1.9212295\ttotal: 104ms\tremaining: 0us\n--- 0.13049793243408203 seconds ---\n"
    },
    {
     "data": {
      "text/plain": "9.344821856482579"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#catboost helps you savetime on preprocessing for categorical columns for you. It is also reportedly faster.\n",
    "\n",
    "#lets try catboost\n",
    "cbr = cb.CatBoostRegressor(learning_rate=0.1,n_estimators=100,max_depth=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cbr.fit(X_train,y_train)\n",
    "\n",
    "y_predict = cbr.predict(X_test)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "mean_squared_error(y_test,y_predict)    #error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Load the intercampusai2019 dataset, train a random forest, extra trees model on the dataset and compare results using both random forests and gradient boosting under the *Gini and Entropy* criterions. \n",
    "\n",
    "<strong>Note: Also make sure to do some data cleaning, upsampling/downsampling, parameter tuning.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_estimators`\n",
    "- increasing num trees wont affect bias, will only reduce variance\n",
    "\n",
    "`max_features`\n",
    "- how many features to split on\n",
    "- rule of thumb = sqrt(num_features)\n",
    "- depends on ratio of noisy to important var in dataset\n",
    "- small num features = reduce variance increase bias\n",
    "- lots of noisy = small m will decrease probability of choosing an important variable at a split\n",
    "\n",
    "`min samples per leaf` \n",
    "- increase a bit (default is 1) to get smaller trees w less overfitting\n",
    "\n",
    "`max_depth`\n",
    "- controls variance\n",
    "\n",
    "`subsample`\n",
    "- The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "- Values slightly less than 1 make the model robust by reducing the variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting point hyperparameters\n",
    "\n",
    "*** Heard from a Kaggle Grandmaster\n",
    "\n",
    "Learning rate = 0.05, 1000 rounds, max depth = 3-5, subsample = 0.8-1.0, colsample_bytree = 0.3 - 0.8, lambda = 0 to 5\n",
    "\n",
    "Add capacity to combat bias - add rounds\n",
    "\n",
    "Reduce capacity to combat variance - depth / regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ]
}